@echo off
echo ğŸš‡ OceanOS LocalTunnel Setup
echo ================================

echo.
echo Current Tunnel URLs:
echo ğŸ“¡ API Server: https://pretty-eyes-join.loca.lt
echo ğŸ¦™ Ollama Service: https://icy-ties-train.loca.lt
echo.

echo ğŸ“ Environment file updated: .env.production
echo ğŸ”§ AI Service configured for tunneled access
echo.

echo ğŸš€ Next Steps:
echo.
echo 1. Keep both tunnels running:
echo    - Ollama: lt --port 11434
echo    - API: npx localtunnel --port 8080
echo.
echo 2. Start your local services:
echo    - Ollama: ollama serve
echo    - OceanOS: npm run dev
echo.
echo 3. Test the tunneled API:
echo    curl https://pretty-eyes-join.loca.lt/api/ping
echo.
echo 4. Deploy to Netlify:
echo    - Build: npm run build
echo    - Deploy with environment variables from .env.production
echo.

echo âœ… Your Netlify site will now access your local Llama model!
echo.

pause